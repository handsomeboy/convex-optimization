\documentclass{paper}

%\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{here}
\usepackage{todonotes}
\usepackage{titlesec}
% load package with ``framed'' and ``numbered'' option.
%\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}

% something NOT relevant to the usage of the package.
\setlength{\parindent}{0pt}
\setlength{\parskip}{18pt}


\usepackage[latin1]{inputenc} 
\usepackage[T1]{fontenc} 

\usepackage{listings} 
\lstset{% 
   language=Matlab, 
   basicstyle=\small\ttfamily, 
} 
\graphicspath{{inpainting/Figures/}}


\title{Image inpainting -- Assignment 1}
\author{Mich\`{e}le Wyss\\10-104-123}
% //////////////////////////////////////////////////

\titleformat*{\section}{\LARGE\bfseries}
\titleformat*{\subsection}{\large\bfseries}
\begin{document}
\maketitle


% Add figures:
%\begin{figure}[t]
%%\begin{center}
%\quad\quad   \includegraphics[width=1\linewidth]{ass2}
%%\end{center}
%
%\label{fig:performance}
%\end{figure}

\section*{Motivation}
Sometimes it's a bad thing that some objects appear in a photograph, e.g. as a tourist I'd like to make a picture of a place but some people are standing around and occlude parts of the actual target image. A simple idea is to just remove the persons after the photograph has been taken. This is a typical example of where an image with holes and invalid regions appears. Image inpainting is one possible approach to generate sensible content for those regions that were occluded before.

\section*{Problem}
Inpainting is an image processing method that aims for reconstruction of lost or corrupted parts of an image. The goal is to get a plausible-looking image without undefined or invalid pieces. Mathematically, the problem can be formulated by defining a cost function that has to be minimized. 
\subsection*{Cost function}
We formulate the following cost function to describe the problem:

$$E(u) = \frac{\lambda}{2} \|u-g\|_\Omega^2 + \|\nabla u \|_2.$$

Here, the value $u$ that we optimize for is an image with sensible content, $g$ is the input image containing undefined pieces, $\lambda$ is a regularization parameter to control the tradeoff between exactness of fitting and smoothness of the result, and $\Omega$ is a mask that defines the missing data. The goal is to find the argument $u$ that minimizes $E$, thus
$$\arg \min_u E(u).$$

\subsection*{Minimization -- Gradient descent}
To minimize the objective function $E$, a simple iterative approach called \emph{gradient descent} is applied. This method proceeds as follows: Starting from an (randomly chosen) initial point, the following update rule is performed:
\begin{equation}
\theta_j := \theta_j - \alpha \cdot \frac{\partial}{\partial \theta_j} E(\theta),
\label{eq:update-rule}
\end{equation}
where $\theta_j$ are the unknown parameters of the model, $\alpha$ is the \emph{learning rate} and $E(\theta)$ is a cost function that is aimed to be minimized, normally called the \emph{objective function}.

In the case of our above formulated objective function, the update rule will look as follows:
$$ u[i,j] := u[i,j] - \alpha \cdot \frac{\partial}{\partial u[i,j]} E(u).$$
\subsection*{Derivation of gradient}
Clearly, we need to know the partial derivative $\frac{\partial}{\partial u[i,j]} E(u)$ to apply gradient descent. It is given by

\begin{align*}
 \frac{\partial}{\partial u[i,j]} \left( \frac{\lambda}{2} \|u-g\|_\Omega^2 + \|\nabla u\|_2 \right) &= \frac{\partial}{\partial u[i,j]} \left( \frac{\lambda}{2} \left(\sum_{i,j} \Omega[i,j] \cdot (u[i,j] - g[i,j])^2\right) + \|\nabla u\|_2 \right) \\
 %
&= \frac{\lambda}{2} \cdot \frac{\partial}{\partial u[i,j]} \left(\sum_{i,j} \Omega[i,j] \cdot (u[i,j] - g[i,j])^2\right) + \frac{\partial}{\partial u[i,j]} \|\nabla u \|_2 \\
%
&= \frac{\lambda}{2} \cdot \frac{\partial}{\partial u[i,j]} \Omega[i,j] \cdot (u[i,j] - g[i,j])^2 + \frac{\partial}{\partial u[i,j]} \|\nabla u \|_2 \\
%
&= \lambda \cdot \Omega[i,j] \cdot (u[i,j] - g[i,j]) + \frac{\partial}{\partial u[i,j]} \|\nabla u \|_2. 
\end{align*}

The target image $u$ is descrete and 2-dimensional. Using the forward difference scheme, we can write $\|\nabla u \|_2$ as

\begin{equation} \|\nabla u \|_2 = \sum_{i,j} \underbrace{\sqrt{(u[i+1,j] - u [i,j])^2 + (u[i,j+1] - u[i,j])^2}}_{\tau[i,j]}.
 \label{eq:grad}
\end{equation}

Because for fixed $i$ and $j$ most of the summation terms in Equation (\ref{eq:grad}) are constant, the exact derivative $\frac{\partial}{\partial u[i,j]} \|\nabla u \|_2$ of the discretized energy reduces to

$$\frac{\partial}{\partial u[i,j]} \|\nabla u \|_2 = \frac{\partial \tau[i,j]}{\partial u[i,j]} + \frac{\partial \tau[i-1,j]}{\partial u[i,j]} + \frac{\partial \tau[i,j-1]}{\partial u[i,j]}.$$

There are now 3 derivative terms that are left to compute. This can be done straight forward:
\begin{align*}
\frac{\partial \tau[i,j]}{\partial u[i,j]} &= \frac{\partial}{\partial u[i,j]} \sqrt{(u[i+1,j] - u[i,j])^2 + (u[i,j+1] - u[i,j])^2} \\
&= \frac{1}{2} \cdot \frac{1}{\tau[i,j]} \cdot \left((-2) \cdot (u[i+1,j] - u[i,j]) -2 (u[i,j+1] - u[i,j])\right) \\
&= \frac{- ((u[i+1,j] - u[i,j]) + (u[i,j+1] - u[i,j]))}{\tau[i,j]} \\
&= \frac{2 u[i,j] - u[i+1,j] - u[i,j+1]}{\tau[i,j]}, \\
\frac{\partial \tau[i-1,j]}{\partial u[i,j]} &= \frac{\partial}{\partial u[i,j]} \sqrt{(u[i,j] - u[i-1,j])^2 + (u[i-1,j+1] - u[i-1,j])^2} \\
&= \frac{1}{2} \cdot \frac{1}{\tau[i-1,j]} \cdot \left( 2 \cdot (u[i,j] - u[i-1,j]) \right) \\
&= \frac{u[i,j]-u[i-1,j]}{\tau[i-1,j]}, \\
\frac{\partial \tau[i,j-1]}{\partial u[i,j]} &= \frac{\partial}{\partial u[i,j]} \sqrt{(u[i+1,j-1] - u[i,j-1])^2 + (u[i,j] - u[i,j-1])^2} \\
&= \frac{1}{2} \cdot \frac{1}{\tau[i,j-1]} \cdot 2 (u[i,j] - u[i,j-1]) \\
&= \frac{u[i,j] - u[i,j-1]}{\tau[i,j-1]}.
\end{align*}

All in all we have therefore the following update rule:
\begin{equation*}
\begin{aligned}
  u[i,j] := u[i,j] - \alpha \cdot \Bigg( \lambda \Omega[i,j] \cdot (u[i,j]-g[i,j]) &+ \frac{2 u[i,j] - u[i+1,j] - u[i,j+1]}{\tau[i,j]} \\ 
  &+ \frac{u[i,j]-u[i-1,j]}{\tau[i-1,j]} \\ 
  &+ \frac{u[i,j] - u[i,j-1]}{\tau[i,j-1]}\Bigg)
\end{aligned}
\label{eq:update-final}
\end{equation*}

\subsection*{Implement gradient descent for inpainting}
I implemented inpainting using the mentioned gradient descent method. The gradients were computed as derived above. 
A quite small learning rate of $\alpha$ = 0.0005 ensures that the iterative approach converges -- however, this also leads to the need of many iterations to get an acceptable result (Figures \ref{fig:results-cat} and \ref{fig:results-hat}). Therefore some of the later shown examples are computed with larger learning rates.
\begin{figure}[ht]
\centering
\begin{subfigure}[h]{0.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{cat-input}
	\caption*{Input}
\end{subfigure}
~
\begin{subfigure}[ht]{0.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{cat-iter100000-lambda1000-alpha0_0005}
	\caption*{Output ($100'000$ iterations)}
\end{subfigure}

\vspace{3mm}
\begin{subfigure}[ht]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{cat-iter1000-lambda1000-alpha0_0005}
	\caption*{$1000$ iterations}
\end{subfigure}
~
\begin{subfigure}[ht]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{cat-iter10000-lambda1000-alpha0_0005}
	\caption*{$10'000$ iterations}
\end{subfigure}
~
\begin{subfigure}[ht]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{cat-iter50000-lambda1000-alpha0_0005}
	\caption*{$50'000$ iterations}
\end{subfigure}
\caption{``Grumpy cat'' -- different stages of the minimization using gradient descent with a learning rate of $\alpha = 0.0005$}
\label{fig:results-cat}
\end{figure}
 
Another example:
\begin{figure}[ht]
\centering
\begin{subfigure}[ht]{0.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hat-input}
	\caption*{Input}
\end{subfigure}
~
\begin{subfigure}[ht]{0.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hat-iter50000-lambda100-alpha0_001}
	\caption*{Output ($50'000$ iterations)}
\end{subfigure}

\vspace{3mm}
\begin{subfigure}[ht]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hat-iter2000-lambda100-alpha0_001}
	\caption*{$2'000$ iterations}
\end{subfigure}
~
\begin{subfigure}[ht]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hat-iter5000-lambda100-alpha0_001}
	\caption*{$5'000$ iterations}
\end{subfigure}
~
\begin{subfigure}[ht]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hat-iter10000-lambda100-alpha0_001}
	\caption*{$10'000$ iterations}
\end{subfigure}
\caption{``Wooly hat'' -- different stages of the minimization using gradient descent with a learning rate of $\alpha = 0.0005$}
\label{fig:results-hat}
\end{figure}

\subsection*{The effect of $\lambda$}
In the objective function
$$E(u) = \frac{\lambda}{2} \|u-g\|_\Omega^2 + \|\nabla u \|_2$$
the parameter $\lambda$ is a \emph{regularization parameter}. It controls the tradeoff between the two constraints of fitting the image and keeping the result smooth. A very low $\lambda$, e.g. $\lambda = 0.1$ leads to an oversmoothed image whereas a very large value for $\lambda$ may possibly fail to reach a nice-looking (smooth) image (see Figure \ref{fig:extreme-lambda-values}). Moreover, a large $\lambda$-value generally requires a smaller learning rate $\alpha$, which can be seen directly by considering the computed final update rule: If $\lambda$ is very large, the first term gets a high weight and may cause the gradient descent method to diverge. Reasonable results can be obtained with $\lambda$-values that lie in between (see Figure \ref{fig:lambda-reasonable}).

\begin{figure}[ht]
 \centering
 \begin{subfigure}[h]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{hat-iter10000-lambda0_1-alpha0_001}
    \caption*{$\lambda = 0.1$, $10'000$ iterations}
 \end{subfigure}
~
 \begin{subfigure}[h]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{hat-iter10000-lambda1990-alpha0_001}
    \caption*{$\lambda = 1'990$, $10'000$ iterations}
 \end{subfigure}
 \caption{Results obtained with extreme values for $\lambda$. Left: low $\lambda$; the result is oversmoothed, Right: high $\lambda$; too much structured}
 \label{fig:extreme-lambda-values}
\end{figure}
\begin{figure}[ht]
 \centering
 \begin{subfigure}[h]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{hat-iter10000-lambda200-alpha0_001}
    \caption*{$\lambda = 200$}
 \end{subfigure}
~
 \begin{subfigure}[h]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{hat-iter10000-lambda500-alpha0_001}
    \caption*{$\lambda = 500$}
 \end{subfigure}
 ~
 \begin{subfigure}[h]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{hat-iter10000-lambda850-alpha0_001}
    \caption*{$\lambda = 850$}
 \end{subfigure}
 \caption{Better results obtained with reasonable values for $\lambda$.}
 \label{fig:lambda-reasonable}
\end{figure}

\subsection*{The optimal $\lambda$}
Performing 1'000 iterations and using a learning rate of $\alpha = 0.01$, we have the following optimal values for $\lambda$: In the case of the wooly hat the optimal value was $\lambda = 53$, in the grumpy cat example it was $\lambda = 51$ which can be seen from the plots in Figure \ref{fig:optimal-lambda-plots}. The vertical axis shows the summed squared distance between the ground truth image and the obtained results, on the horizontal axis is $\lambda$.
\begin{figure}[ht]
\centering
\begin{subfigure}[h]{0.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{lambdaVSssd-iter1000-alpha0_01-grumpycat}
	\caption*{Grumpy cat}
\end{subfigure}
~
\begin{subfigure}[h]{0.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{lambdaVSssd-iter1000-alpha0_01-woolyhat}
	\caption*{Wooly hat}
\end{subfigure}
\caption{$\lambda$ vs. $SSD$ plot. The vertical line denotes the optimal value for $\lambda$.}
\label{fig:optimal-lambda-plots}
\end{figure}

\begin{figure}[ht]
\centering
\begin{subfigure}[h]{0.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{cat-groundtruth}
	\caption*{ground truth}
\end{subfigure}
~
\begin{subfigure}[h]{0.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{cat-iter1000-lambda51-alpha0_01}
	\caption*{$\lambda = 51$, $1'000$ iterations}
\end{subfigure}

\vspace{3mm}
\begin{subfigure}[h]{0.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hat-groundtruth}
	\caption*{ground truth}
\end{subfigure}
~
\begin{subfigure}[h]{0.45\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hat-iter2000-lambda53-alpha0_01}
	\caption*{$\lambda = 53$, $2'000$ iterations}
\end{subfigure}
\caption{Top row: The optimal $\lambda$ leads to reasonable results. Bottom row: The optimal $\lambda$ does not lead to satisfying quality of the resulting image. Choosing a lower learning rate leads to better results. This will, however, also lead to another optimal value for $\lambda$.}
\label{fig:optimal-lambda-images}
\end{figure}
 \end{document}
 
 